{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch.functional import F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import torch_geometric.nn\n",
    "\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FLOOD_KEEP = [\n",
    "    \"region\",\n",
    "    \"in_flom_analyseomraade\",\n",
    "    \"hazard_class_flom\",\n",
    "]\n",
    "\n",
    "LANDSLIDE_CLOSEST_KEEP = [\n",
    "    \"inside_source_area\",\n",
    "    \"inside_runout_area\",\n",
    "    \"any_landslide_area_inside\",\n",
    "    \"dist_to_trigger_point\",\n",
    "    \"dist_to_runout_point\",\n",
    "    \"dist_to_source_area\",\n",
    "    \"dist_to_runout_area\",\n",
    "    \"dist_to_landslide_event\",\n",
    "    \"sourcearea_area_m2\",\n",
    "    \"runoutarea_area_m2\",\n",
    "    \"trigger_point_far\",\n",
    "    \"runout_point_far\",\n",
    "    \"landslide_event_far\",\n",
    "]\n",
    "\n",
    "LANDSLIDE_EVENT_RAW_KEEP = [\n",
    "    \"skredID\",\n",
    "    \"distance_m\",\n",
    "    \"skredType\",\n",
    "    \"skredTidspunkt\",\n",
    "]\n",
    "\n",
    "HYDRO_KEEP = [\n",
    "    \"dist_to_river\",\n",
    "    \"dist_to_lake\",\n",
    "    \"dist_to_hyd\",\n",
    "    \"arealEnhet_km2\",\n",
    "    \"arealTotal_km2\",\n",
    "    \"QNormal_lskm2\",\n",
    "    \"QNormal_Mm3Aar\",\n",
    "    \"QNormalOppstrm_Mm3Aar\",\n",
    "    \"elveordenstrahler\",\n",
    "    \"arealregineenhet_km2\",\n",
    "    \"areal_km2\",\n",
    "    \"arealNorge_km2\",\n",
    "    \"nedborfeltareal_km2\",\n",
    "    \"minsteVannforing\",\n",
    "]\n",
    "\n",
    "\n",
    "def aggregate_per_house(df, house_id_col=\"bygningsnummer\"):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    if house_id_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    num_cols = [\n",
    "        c for c in df.columns\n",
    "        if c != house_id_col and pd.api.types.is_numeric_dtype(df[c])\n",
    "    ]\n",
    "    bool_cols = [\n",
    "        c for c in df.columns\n",
    "        if c != house_id_col and df[c].dtype == \"bool\"\n",
    "    ]\n",
    "    other_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in ([house_id_col] + num_cols + bool_cols)\n",
    "    ]\n",
    "\n",
    "    agg = {}\n",
    "    for c in num_cols:\n",
    "        agg[c] = \"median\"\n",
    "    for c in bool_cols:\n",
    "        agg[c] = \"max\"     \n",
    "    for c in other_cols:\n",
    "        agg[c] = \"first\"   \n",
    "\n",
    "    out = df.groupby(house_id_col, as_index=False).agg(agg)\n",
    "    return out\n",
    "\n",
    "\n",
    "def keep_only(df: pd.DataFrame, keep: list[str], *, always_keep: str | None = None) -> pd.DataFrame:\n",
    "    keep_set = set(keep)\n",
    "    if always_keep:\n",
    "        keep_set.add(always_keep)\n",
    "\n",
    "    cols = [c for c in df.columns if c in keep_set]\n",
    "    return df.loc[:, cols].copy()\n",
    "\n",
    "\n",
    "def encode_raw_landslides(\n",
    "    landslides_events: pd.DataFrame,\n",
    "    house_df: pd.DataFrame,\n",
    "    house_id_col=\"bygningsnummer\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    0 = no events\n",
    "    1 = one event\n",
    "    2 = two+ events\n",
    "    \"\"\"\n",
    "    if landslides_events is None or landslides_events.empty:\n",
    "        out = house_df[[house_id_col]].copy()\n",
    "        out[\"landslide_hazard_level\"] = 0\n",
    "        return out\n",
    "\n",
    "    counts = landslides_events[house_id_col].value_counts()\n",
    "    hazard = counts.apply(lambda x: 2 if x > 1 else 1)\n",
    "\n",
    "    hazard_df = hazard.reset_index()\n",
    "    hazard_df.columns = [house_id_col, \"landslide_hazard_level\"]\n",
    "\n",
    "    merged = house_df[[house_id_col]].merge(hazard_df, on=house_id_col, how=\"left\")\n",
    "    merged[\"landslide_hazard_level\"] = merged[\"landslide_hazard_level\"].fillna(0).astype(int)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def clean_link_tables(\n",
    "    flood_links: pd.DataFrame | None = None,\n",
    "    landslide_closest: pd.DataFrame | None = None,\n",
    "    landslide_events_raw: pd.DataFrame | None = None,\n",
    "    hydro_links: pd.DataFrame | None = None,\n",
    "    *,\n",
    "    house_id_col: str = \"bygningsnummer\",\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    if flood_links is not None:\n",
    "        out[\"flood\"] = keep_only(flood_links, FLOOD_KEEP, always_keep=house_id_col)\n",
    "\n",
    "    if landslide_closest is not None:\n",
    "        out[\"landslide_closest\"] = keep_only(landslide_closest, LANDSLIDE_CLOSEST_KEEP, always_keep=house_id_col)\n",
    "\n",
    "    if landslide_events_raw is not None:\n",
    "        out[\"landslide_events_raw\"] = keep_only(landslide_events_raw, LANDSLIDE_EVENT_RAW_KEEP, always_keep=house_id_col)\n",
    "\n",
    "    if hydro_links is not None:\n",
    "        out[\"hydro\"] = keep_only(hydro_links, HYDRO_KEEP, always_keep=house_id_col)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_houses_data(\n",
    "    region: str,\n",
    "    base_dir: str | Path = \"master\",\n",
    "    target_epsg: int = 25833,\n",
    "    house_id_col: str = \"bygningsnummer\",\n",
    "    use_landslide_closest: bool = True,\n",
    "):\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    houses_path = base_dir / f\"raw/vector/houses/houses_{region}.gpkg\"\n",
    "    houses_layer = f\"houses_{region}\"\n",
    "\n",
    "    houses = gpd.read_file(houses_path, layer=houses_layer).to_crs(epsg=target_epsg)\n",
    "    houses = houses.set_geometry(\"geometry\")\n",
    "    houses[house_id_col] = houses[house_id_col].astype(\"int64\")\n",
    "\n",
    "    # link tables\n",
    "    f_links = pd.read_parquet(base_dir / f\"processed/links/flood_links_{region}.parquet\")\n",
    "    h_links = pd.read_parquet(base_dir / f\"processed/links/hydro_links_{region}.parquet\")\n",
    "\n",
    "    # closest landslide table is optional\n",
    "    l_closest = None\n",
    "    if use_landslide_closest:\n",
    "        l_closest_path = base_dir / f\"processed/links/landslide_links_closest_{region}.parquet\"\n",
    "        if l_closest_path.exists():\n",
    "            l_closest = pd.read_parquet(l_closest_path)\n",
    "\n",
    "    # raw landslide events -> used for encoding target\n",
    "    l_events = pd.read_parquet(base_dir / f\"processed/links/landslide_links_{region}.parquet\")\n",
    "\n",
    "    cleaned = clean_link_tables(\n",
    "        flood_links=f_links,\n",
    "        landslide_closest=l_closest,\n",
    "        landslide_events_raw=l_events,\n",
    "        hydro_links=h_links,\n",
    "        house_id_col=house_id_col,\n",
    "    )\n",
    "\n",
    "    f_clean = aggregate_per_house(cleaned[\"flood\"], house_id_col)\n",
    "    h_clean = aggregate_per_house(cleaned[\"hydro\"], house_id_col)\n",
    "    l_closest_clean = aggregate_per_house(cleaned.get(\"landslide_closest\"), house_id_col)\n",
    "\n",
    "    l_events_clean = cleaned.get(\"landslide_events_raw\")\n",
    "    l_encoded = encode_raw_landslides(l_events_clean, houses, house_id_col=house_id_col)\n",
    "    l_encoded = aggregate_per_house(l_encoded, house_id_col)\n",
    "\n",
    "    # merge links safely\n",
    "    link_tables = [f_clean, h_clean, l_encoded]\n",
    "    if l_closest_clean is not None:\n",
    "        link_tables.append(l_closest_clean)\n",
    "\n",
    "    links = link_tables[0]\n",
    "    for nxt in link_tables[1:]:\n",
    "        links = links.merge(nxt, on=house_id_col, how=\"left\")\n",
    "\n",
    "    houses_gdf = houses.merge(links, on=house_id_col, how=\"left\")\n",
    "    houses_gdf = gpd.GeoDataFrame(houses_gdf, geometry=\"geometry\", crs=houses.crs)\n",
    "\n",
    "    return houses_gdf\n",
    "\n",
    "\n",
    "def build_knn_edge_index(coords: np.ndarray, k: int = 8, make_undirected: bool = True):\n",
    "    if len(coords) == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    n_neighbors = min(k + 1, len(coords))\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"auto\")\n",
    "    nbrs.fit(coords)\n",
    "    _, idxs = nbrs.kneighbors(coords)\n",
    "\n",
    "    src_list, dst_list = [], []\n",
    "    for i in range(len(coords)):\n",
    "        neigh = idxs[i][1:]  # remove self\n",
    "        for j in neigh:\n",
    "            src_list.append(i)\n",
    "            dst_list.append(j)\n",
    "\n",
    "    edge_index = torch.tensor([src_list, dst_list], dtype=torch.long)\n",
    "\n",
    "    if make_undirected and edge_index.numel() > 0:\n",
    "        rev = edge_index.flip(0)\n",
    "        edge_index = torch.cat([edge_index, rev], dim=1)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_and_save_knn_graph(\n",
    "    houses_gdf,\n",
    "    out_path,\n",
    "    house_id_col=\"bygningsnummer\",\n",
    "    k=8,\n",
    "):\n",
    "    houses = houses_gdf[[house_id_col, \"geometry\"]].copy()\n",
    "    houses[house_id_col] = houses[house_id_col].astype(int)\n",
    "    houses = houses.dropna(subset=[\"geometry\"])\n",
    "    houses = houses.sort_values(house_id_col).reset_index(drop=True)\n",
    "\n",
    "    coords = np.array([(g.x, g.y) for g in houses.geometry], dtype=np.float32)\n",
    "    pos = torch.from_numpy(coords)\n",
    "\n",
    "    edge_index = build_knn_edge_index(coords, k=k)\n",
    "\n",
    "    house_id = torch.tensor(houses[house_id_col].to_numpy(), dtype=torch.long)\n",
    "\n",
    "    payload = {\n",
    "        \"house_id\": house_id,\n",
    "        \"pos\": pos,\n",
    "        \"edge_index\": edge_index,\n",
    "        \"k\": k,\n",
    "        \"house_id_col\": house_id_col,\n",
    "    }\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(payload, out_path)\n",
    "    print(\"Saved graph:\", out_path, \"| nodes:\", len(houses))\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def load_graph_bundle(path):\n",
    "    return torch.load(Path(path), map_location=\"cpu\")\n",
    "\n",
    "\n",
    "def default_raster_paths_for_region(region, base_dir=\"../\"):\n",
    "    base_dir = Path(base_dir)\n",
    "    raw_raster_dir = base_dir / \"raw\" / \"rasters\"\n",
    "    processed_raster_dir = base_dir / \"processed\" / \"rasters\"\n",
    "\n",
    "    return {\n",
    "        \"dtm\": raw_raster_dir / f\"{region}_dtm10.tif\",\n",
    "        \"slope\": processed_raster_dir / f\"{region}_slope_deg.tif\",\n",
    "        \"aspect\": processed_raster_dir / f\"{region}_aspect_deg.tif\",\n",
    "        \"curv\": processed_raster_dir / f\"{region}_curvature.tif\",\n",
    "        \"flowacc\": processed_raster_dir / f\"{region}_flowacc_d8.tif\",\n",
    "        \"twi\": processed_raster_dir / f\"{region}_twi.tif\",\n",
    "    }\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def build_data_objects(\n",
    "    region,\n",
    "    house_id_col=\"bygningsnummer\",\n",
    "    base_dir=\"../\",\n",
    "    flood_label=\"hazard_class_flom\",\n",
    "    landslide_label=\"landslide_hazard_level\",\n",
    "    graph_path=None,\n",
    "):\n",
    "    houses_gdf = load_houses_data(\n",
    "        region,\n",
    "        base_dir=base_dir,\n",
    "    )\n",
    "    houses_gdf = houses_gdf.set_geometry(\"geometry\").copy()\n",
    "    houses_gdf[house_id_col] = houses_gdf[house_id_col].astype(int)\n",
    "\n",
    "    graph = load_graph_bundle(graph_path)\n",
    "    graph_ids = graph[\"house_id\"].cpu().numpy().tolist()\n",
    "\n",
    "    # 1) filter houses to graph ids\n",
    "    houses = houses_gdf[houses_gdf[house_id_col].isin(graph_ids)].copy()\n",
    "\n",
    "    # 2) enforce EXACT order = graph house_id order\n",
    "    order_map = {hid: i for i, hid in enumerate(graph_ids)}\n",
    "    houses[\"__order\"] = houses[house_id_col].map(order_map)\n",
    "    houses = houses.sort_values(\"__order\").drop(columns=\"__order\").reset_index(drop=True)\n",
    "\n",
    "    # sanity\n",
    "    assert len(houses) == len(graph_ids), f\"Graph nodes ({len(graph_ids)}) != houses ({len(houses)})\"\n",
    "\n",
    "    # 3) build numeric features\n",
    "    df_for_features = pd.DataFrame(houses.drop(columns=[\"geometry\"], errors=\"ignore\"))\n",
    "\n",
    "    drop_always = {house_id_col, \"bygningId\", \"gml_id\", \"uuidBygning\", \"versjonId\"}\n",
    "    drop_labels = {flood_label, landslide_label}\n",
    "\n",
    "    num_df = df_for_features.select_dtypes(include=[np.number]).copy()\n",
    "    num_df = num_df[[c for c in num_df.columns if c not in (drop_always | drop_labels)]]\n",
    "    num_df = num_df.dropna(axis=1, how=\"all\")\n",
    "    num_df = num_df.fillna(num_df.median(numeric_only=True))\n",
    "\n",
    "    x = torch.tensor(num_df.to_numpy(dtype=np.float32))\n",
    "\n",
    "    # 4) labels + masks\n",
    "    y_flood_t = None\n",
    "    y_landslide_t = None\n",
    "    y_flood_mask_t = None\n",
    "    y_landslide_mask_t = None\n",
    "\n",
    "    if flood_label in df_for_features.columns:\n",
    "        y_f = df_for_features[flood_label].to_numpy()\n",
    "        flood_mask = ~pd.isna(y_f)\n",
    "        y_f = pd.Series(y_f).fillna(-1).astype(int).to_numpy()\n",
    "        y_flood_t = torch.tensor(y_f, dtype=torch.long)\n",
    "        y_flood_mask_t = torch.tensor(flood_mask, dtype=torch.bool)\n",
    "\n",
    "    if landslide_label in df_for_features.columns:\n",
    "        y_l = df_for_features[landslide_label].to_numpy()\n",
    "        landslide_mask = ~pd.isna(y_l)\n",
    "        y_l = pd.Series(y_l).fillna(-1).astype(int).to_numpy()\n",
    "        y_landslide_t = torch.tensor(y_l, dtype=torch.long)\n",
    "        y_landslide_mask_t = torch.tensor(landslide_mask, dtype=torch.bool)\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        pos=graph[\"pos\"],\n",
    "        edge_index=graph[\"edge_index\"],\n",
    "        house_id=graph[\"house_id\"],\n",
    "    )\n",
    "\n",
    "    if y_flood_t is not None:\n",
    "        data.y_flood = y_flood_t\n",
    "        data.y_flood_mask = y_flood_mask_t\n",
    "\n",
    "    if y_landslide_t is not None:\n",
    "        data.y_landslide = y_landslide_t\n",
    "        data.y_landslide_mask = y_landslide_mask_t\n",
    "\n",
    "    return data, houses, num_df.columns.tolist(), graph\n",
    "\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "\n",
    "def add_splits(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    splitter = RandomNodeSplit(\n",
    "        num_val=int(val_ratio * data.num_nodes),\n",
    "        num_test=int(test_ratio * data.num_nodes),\n",
    "    )\n",
    "    data = splitter(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class TerrainBuilderStack:\n",
    "    def __init__(self, region, raster_paths, houses_gdf, patches_m=200):\n",
    "        self.region = region\n",
    "        self.raster_paths = raster_paths  # dict: {\"dtm\": Path, ...}\n",
    "        self.houses_gdf = houses_gdf\n",
    "        self.patches_m = patches_m\n",
    "        self._sources = None\n",
    "        self.label_flood = \"hazard_class_flom\"\n",
    "        self.label_landslide = \"landslide_hazard_level\"\n",
    "\n",
    "    def _open_sources(self):\n",
    "        if self._sources is None:\n",
    "            keys = [\"dtm\", \"slope\", \"aspect\", \"curv\", \"flowacc\", \"twi\"]\n",
    "            self._sources = {k: rasterio.open(self.raster_paths[k]) for k in keys}\n",
    "\n",
    "            # ensure house CRS matches rasters\n",
    "            dtm_crs = self._sources[\"dtm\"].crs\n",
    "            if self.houses_gdf.crs != dtm_crs:\n",
    "                self.houses_gdf = self.houses_gdf.to_crs(dtm_crs)\n",
    "\n",
    "        return self._sources\n",
    "\n",
    "    def close(self):\n",
    "        if self._sources:\n",
    "            for src in self._sources.values():\n",
    "                src.close()\n",
    "        self._sources = None\n",
    "\n",
    "    def extract_one_patch(self, geom):\n",
    "        srcs = self._open_sources()\n",
    "        dtm_src = srcs[\"dtm\"]\n",
    "\n",
    "        res_x, res_y = dtm_src.res\n",
    "        assert abs(res_x - res_y) < 1e-6, \"Rasters must have square pixels\"\n",
    "        res = float(res_x)\n",
    "\n",
    "        half_pixels = int((self.patches_m / 2) / res)\n",
    "        patch_pixels = half_pixels * 2\n",
    "\n",
    "        row_idx, col_idx = dtm_src.index(geom.x, geom.y)\n",
    "\n",
    "        window = Window(\n",
    "            col_idx - half_pixels,\n",
    "            row_idx - half_pixels,\n",
    "            patch_pixels,\n",
    "            patch_pixels,\n",
    "        )\n",
    "\n",
    "        keys = [\"dtm\", \"slope\", \"aspect\", \"curv\", \"flowacc\", \"twi\"]\n",
    "        patches = [srcs[k].read(1, window=window) for k in keys]\n",
    "\n",
    "        # Skip edge cases\n",
    "        if any(p.shape != (patch_pixels, patch_pixels) for p in patches):\n",
    "            return None\n",
    "\n",
    "        stack = np.stack(patches, axis=0).astype(\"float32\")\n",
    "\n",
    "        stack = np.where(np.isfinite(stack), stack, np.nan)\n",
    "        for c in range(stack.shape[0]):\n",
    "            med = np.nanmedian(stack[c])\n",
    "            if np.isnan(med):\n",
    "                med = 0.0\n",
    "            stack[c] = np.where(np.isnan(stack[c]), med, stack[c])\n",
    "\n",
    "        return stack\n",
    "\n",
    "\n",
    "class TerrainEncoderCNN(nn.Module):\n",
    "    def __init__(self, in_channels=6, t_dim=128):\n",
    "        super(TerrainEncoderCNN, self).__init__()\n",
    "        import torch.nn as nn\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, bias=False, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 96, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(96, 96, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(96, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, t_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, in_dim, d_tab):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        import torch.nn as nn\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Linear(128, d_tab),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class CombinedMLP(nn.Module):\n",
    "    def __init__(self, d_terrain=128, d_tab=64, d_node=192):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_terrain + d_tab, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, d_node),\n",
    "        )\n",
    "\n",
    "    def forward(self, z_terrain, z_tab):\n",
    "        return self.net(torch.cat([z_terrain, z_tab], dim=-1))\n",
    "\n",
    "from torch_geometric.nn import GCNConv  \n",
    "class GNNBackbone(nn.Module):\n",
    "    def __init__(self, in_dim=192, hidden_dim=192, num_layers=3):\n",
    "        super(GNNBackbone, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class HazardHeads(nn.Module):\n",
    "    def __init__(self, in_dim, num_flood_classes=3):\n",
    "        super(HazardHeads, self).__init__()\n",
    "        self.flood_head = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_flood_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        flood_logits = self.flood_head(x)\n",
    "        return flood_logits\n",
    "\n",
    "\n",
    "class HazardGNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        terrain_in_channels=6,\n",
    "        terrain_dim=128,\n",
    "        tabular_in_dim=50,\n",
    "        tabular_dim=64,\n",
    "        gnn_hidden_dim=192,\n",
    "        gnn_layers=3,\n",
    "        num_flood_classes=3,\n",
    "    ):\n",
    "        super(HazardGNNModel, self).__init__()\n",
    "\n",
    "        self.terrain_encoder = TerrainEncoderCNN(\n",
    "            in_channels=terrain_in_channels,\n",
    "            t_dim=terrain_dim\n",
    "        )\n",
    "        self.tabular_mlp = TabularMLP(\n",
    "            in_dim=tabular_in_dim,\n",
    "            d_tab=tabular_dim\n",
    "        )\n",
    "        self.combined_mlp = CombinedMLP(\n",
    "            d_terrain=terrain_dim,\n",
    "            d_tab=tabular_dim,\n",
    "            d_node=gnn_hidden_dim\n",
    "        )\n",
    "\n",
    "        # âœ… FIXED: input dim matches combined output\n",
    "        self.gnn_backbone = GNNBackbone(\n",
    "            in_dim=gnn_hidden_dim,\n",
    "            hidden_dim=gnn_hidden_dim,\n",
    "            num_layers=gnn_layers\n",
    "        )\n",
    "\n",
    "        self.hazard_heads = HazardHeads(\n",
    "            in_dim=gnn_hidden_dim,\n",
    "            num_flood_classes=num_flood_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, data, patches, node_idx):\n",
    "        z_terrain = self.terrain_encoder(patches)\n",
    "        z_tab = self.tabular_mlp(data.x[node_idx])\n",
    "        combined = self.combined_mlp(z_terrain, z_tab)\n",
    "\n",
    "        x_full = data.x.new_zeros((data.num_nodes, combined.size(-1)))\n",
    "        x_full[node_idx] = combined\n",
    "\n",
    "        gnn_feats = self.gnn_backbone(x_full, data.edge_index)\n",
    "        out = gnn_feats[node_idx]\n",
    "\n",
    "        flood_logits = self.hazard_heads(out)\n",
    "        return flood_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HouseGraphDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        region,\n",
    "        base_dir=\"../\",\n",
    "        k=8,\n",
    "        flood_label=\"hazard_class_flom\",\n",
    "        landslide_label=\"landslide_hazard_level\",\n",
    "        graph_path=\"\",\n",
    "        patches_m=100,\n",
    "    ):\n",
    "        self.region = region\n",
    "        self.base_dir = base_dir\n",
    "        self.k = k\n",
    "        self.flood_label = flood_label\n",
    "        self.landslide_label = landslide_label\n",
    "        self.graph_path = graph_path\n",
    "        self.patches_m = patches_m\n",
    "\n",
    "        self.raster_paths = default_raster_paths_for_region(region, base_dir=base_dir)\n",
    "\n",
    "        data, houses_ok, feature_cols, graph = build_data_objects(\n",
    "            region,\n",
    "            house_id_col=\"bygningsnummer\",\n",
    "            base_dir=base_dir,\n",
    "            flood_label=flood_label,\n",
    "            landslide_label=landslide_label,\n",
    "            graph_path=graph_path,\n",
    "        )\n",
    "\n",
    "        self.data = data\n",
    "        self.houses_ok = houses_ok\n",
    "        self.feature_cols = feature_cols\n",
    "        self.graph = graph\n",
    "\n",
    "        self.terrain = TerrainBuilderStack(\n",
    "            region=region,\n",
    "            raster_paths=self.raster_paths,\n",
    "            houses_gdf=self.houses_ok,\n",
    "            patches_m=patches_m,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data\n",
    "\n",
    "    def get_patches_for_node_idx(self, node_idx_tensor):\n",
    "        patches = []\n",
    "        for i in node_idx_tensor.tolist():\n",
    "            geom = self.houses_ok.iloc[int(i)].geometry\n",
    "            patch = self.terrain.extract_one_patch(geom)\n",
    "\n",
    "            if patch is None:\n",
    "                # fallback zeros\n",
    "                # infer size from dtm resolution\n",
    "                dtm_src = self.terrain._open_sources()[\"dtm\"]\n",
    "                res = float(dtm_src.res[0])\n",
    "                half = int((self.patches_m / 2) / res)\n",
    "                px = half * 2\n",
    "                patch = np.zeros((6, px, px), dtype=\"float32\")\n",
    "\n",
    "            patches.append(torch.from_numpy(patch).float())\n",
    "\n",
    "        return torch.stack(patches, dim=0)\n",
    "\n",
    "    def close(self):\n",
    "        self.terrain.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def masked_ce_loss(logits, y, mask):\n",
    "    if logits is None or y is None or mask is None:\n",
    "        return 0.0\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return nn.CrossEntropyLoss()(logits[mask], y[mask])\n",
    "\n",
    "\n",
    "def _index_from_mask(mask):\n",
    "    return mask.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "\n",
    "def _safe_masked_idx(idx, mask):\n",
    "    \"\"\"Return subset of idx where mask is True.\"\"\"\n",
    "    if mask is None:\n",
    "        return idx\n",
    "    return idx[mask[idx]]\n",
    "\n",
    "\n",
    "def train_epoch_node_batches(\n",
    "    model,\n",
    "    dataset,          # HouseGraphDataset\n",
    "    train_idx,        # tensor of node indices\n",
    "    optimizer,\n",
    "    loss_fn_flood,\n",
    "    loss_fn_landslide,\n",
    "    device,\n",
    "    node_batch_size=256,\n",
    "    steps_per_epoch=200,\n",
    "):\n",
    "    model.train()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    # These may or may not exist depending on your build_data_objects\n",
    "    flood_mask = getattr(data, \"y_flood_mask\", None)\n",
    "    ls_mask    = getattr(data, \"y_landslide_mask\", None)\n",
    "\n",
    "    for _ in tqdm(range(steps_per_epoch), desc=\"train steps\"):\n",
    "        if train_idx.numel() == 0:\n",
    "            break\n",
    "\n",
    "        # sample node batch from train_idx\n",
    "        perm = torch.randperm(train_idx.numel())\n",
    "        node_idx = train_idx[perm[:min(node_batch_size, train_idx.numel())]].to(device)\n",
    "\n",
    "        # load patches only for these nodes\n",
    "        patches = dataset.get_patches_for_node_idx(node_idx.cpu()).to(device)\n",
    "\n",
    "        flood_logits = model(data, patches, node_idx)\n",
    "\n",
    "        # ----- flood loss -----\n",
    "        lf = 0.0\n",
    "        if hasattr(data, \"y_flood\"):\n",
    "            valid_f_idx = _safe_masked_idx(node_idx, flood_mask)\n",
    "            if valid_f_idx.numel() > 0:\n",
    "                # map valid_f_idx -> positions inside node_idx\n",
    "                # easiest: build a mask relative to node_idx\n",
    "                rel_mask = torch.isin(node_idx, valid_f_idx)\n",
    "                y_f = data.y_flood[node_idx][rel_mask]\n",
    "                pred_f = flood_logits[rel_mask]\n",
    "                lf = loss_fn_flood(pred_f, y_f)\n",
    "\n",
    "        # ----- landslide loss -----\n",
    "        ll = 0.0\n",
    "\n",
    "        # if both missing, skip step\n",
    "        if isinstance(lf, float) and isinstance(ll, float) and lf == 0.0 and ll == 0.0:\n",
    "            continue\n",
    "\n",
    "        loss = lf + ll\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "    return total_loss / max(n_steps, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch_node_batches(\n",
    "    model,\n",
    "    dataset,\n",
    "    eval_idx,\n",
    "    loss_fn_flood,\n",
    "    loss_fn_landslide,\n",
    "    device,\n",
    "    node_batch_size=512,\n",
    "):\n",
    "    model.eval()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    flood_mask = getattr(data, \"y_flood_mask\", None)\n",
    "    ls_mask    = getattr(data, \"y_landslide_mask\", None)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    flood_correct = 0\n",
    "    flood_total = 0\n",
    "\n",
    "    ls_correct = 0\n",
    "    ls_total = 0\n",
    "\n",
    "    # chunk eval_idx\n",
    "    for start in tqdm(range(0, eval_idx.numel(), node_batch_size), desc=\"eval batches\"):\n",
    "        node_idx = eval_idx[start:start + node_batch_size].to(device)\n",
    "        if node_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        patches = dataset.get_patches_for_node_idx(node_idx.cpu()).to(device)\n",
    "        flood_logits = model(data, patches, node_idx)\n",
    "\n",
    "        lf = 0.0\n",
    "        ll = 0.0\n",
    "\n",
    "        # flood metrics\n",
    "        if hasattr(data, \"y_flood\"):\n",
    "            valid_f_idx = _safe_masked_idx(node_idx, flood_mask)\n",
    "            if valid_f_idx.numel() > 0:\n",
    "                rel_mask = torch.isin(node_idx, valid_f_idx)\n",
    "                y_f = data.y_flood[node_idx][rel_mask]\n",
    "                pred_f = flood_logits[rel_mask]\n",
    "                lf = loss_fn_flood(pred_f, y_f)\n",
    "\n",
    "                flood_preds = pred_f.argmax(dim=1)\n",
    "                flood_correct += (flood_preds == y_f).sum().item()\n",
    "                flood_total += y_f.numel()\n",
    "\n",
    "        # landslide metrics\n",
    "\n",
    "        if isinstance(lf, float) and isinstance(ll, float) and lf == 0.0 and ll == 0.0:\n",
    "            continue\n",
    "\n",
    "        loss = lf + ll\n",
    "        total_loss += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "    flood_acc = flood_correct / flood_total if flood_total > 0 else 0.0\n",
    "    ls_acc    = ls_correct / ls_total if ls_total > 0 else 0.0\n",
    "\n",
    "    return (total_loss / max(n_steps, 1)), flood_acc, ls_acc\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "ds = HouseGraphDataset(\n",
    "    region=\"sogn\",\n",
    "    base_dir=\"../\",\n",
    "    graph_path=\"../processed/graphs/knn_graph_sogn_k8.pt\",\n",
    "    patches_m=100,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = ds.data\n",
    "graph = torch.load(\"../processed/graphs/knn_graph_sogn_k8.pt\", map_location=\"cpu\")\n",
    "train_idx = graph.get(\"train_idx\", None)\n",
    "val_idx   = graph.get(\"val_idx\", None)\n",
    "test_idx  = graph.get(\"test_idx\", None)\n",
    "\n",
    "# Fallback if you didn't save splits yet:\n",
    "if train_idx is None:\n",
    "    # simplest fallback using masks if they exist\n",
    "    if hasattr(data, \"train_mask\"):\n",
    "        train_idx = _index_from_mask(data.train_mask)\n",
    "        val_idx   = _index_from_mask(data.val_mask)\n",
    "        test_idx  = _index_from_mask(data.test_mask)\n",
    "    else:\n",
    "        # last-resort quick split over all nodes\n",
    "        N = data.num_nodes\n",
    "        perm = torch.randperm(N)\n",
    "        n_test = int(0.1 * N)\n",
    "        n_val  = int(0.1 * N)\n",
    "        test_idx = perm[:n_test]\n",
    "        val_idx  = perm[n_test:n_test+n_val]\n",
    "        train_idx = perm[n_test+n_val:]\n",
    "\n",
    "train_idx = train_idx.cpu()\n",
    "val_idx   = val_idx.cpu()\n",
    "test_idx  = test_idx.cpu()\n",
    "\n",
    "# Model\n",
    "tabular_in_dim = data.x.size(1)\n",
    "\n",
    "model = HazardGNNModel(\n",
    "    terrain_in_channels=6,\n",
    "    terrain_dim=128,\n",
    "    tabular_in_dim=tabular_in_dim,\n",
    "    tabular_dim=64,\n",
    "    gnn_hidden_dim=192,\n",
    "    gnn_layers=3,\n",
    "    num_flood_classes=3,\n",
    ").to(device)\n",
    "\n",
    "# Losses (keep separate so you can weight later if you want)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "loss_l = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, 21):\n",
    "    train_loss = train_epoch_node_batches(\n",
    "        model=model,\n",
    "        dataset=ds,\n",
    "        train_idx=train_idx,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn_flood=loss_f,\n",
    "        loss_fn_landslide=loss_l,\n",
    "        device=device,\n",
    "        node_batch_size=256,\n",
    "        steps_per_epoch=200,\n",
    "    )\n",
    "\n",
    "    val_loss, val_f_acc, val_l_acc = eval_epoch_node_batches(\n",
    "        model=model,\n",
    "        dataset=ds,\n",
    "        eval_idx=val_idx,\n",
    "        loss_fn_flood=loss_f,\n",
    "        loss_fn_landslide=loss_l,\n",
    "        device=device,\n",
    "        node_batch_size=512,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} | \"\n",
    "        f\"val_loss={val_loss:.4f} | \"\n",
    "        f\"val_f_acc={val_f_acc:.3f} | \"\n",
    "        f\"val_l_acc={val_l_acc:.3f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "georisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
