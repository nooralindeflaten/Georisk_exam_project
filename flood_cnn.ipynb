{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Flood CNN's for single region Årdal and Multi region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Årdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "LABELS_PATH = Path(\"data/processed/features/houses_ardal_flom_labels.parquet\")\n",
    "RASTER_DIR  = Path(\"data/derived/rasters\")\n",
    "OUT_DIR     = Path(\"data/derived/dtm_features_ardal10m\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DTM_PATH      = RASTER_DIR / \"ardal_dtm10_25833.tif\"\n",
    "SLOPE_PATH    = RASTER_DIR / \"ardal_slope_deg_25833.tif\"\n",
    "ASPECT_PATH   = RASTER_DIR / \"ardal_aspect_deg_25833.tif\"\n",
    "CURV_PATH     = RASTER_DIR / \"ardal_curvature_25833.tif\"\n",
    "FLOWACC_PATH  = RASTER_DIR / \"ardal_flowacc_25833.tif\"\n",
    "TWI_PATH      = RASTER_DIR / \"ardal_twi.tif\"\n",
    "HOUSES_GPKG = Path(\"../data/processed/vector/exposure/houses_ardal_flom.gpkg\")\n",
    "HOUSES_LAYER = \"houses_flom\"\n",
    "PATCH_SIZE_M = 200.0  # 200x200 m around each house\n",
    "\n",
    "def build_house_stacks():\n",
    "    df = gpd.read_file(HOUSES_GPKG, layer=HOUSES_LAYER).to_crs(25833)\n",
    "    df.set_geometry(\"geometry\")\n",
    "    df = df[df[\"hazard_class_flom\"].notna()].copy()\n",
    "    df[\"bygningsnummer\"] = df[\"bygningsnummer\"].astype(int)\n",
    "\n",
    "    with rasterio.open(DTM_PATH) as dem_src, \\\n",
    "         rasterio.open(SLOPE_PATH) as slope_src, \\\n",
    "         rasterio.open(ASPECT_PATH) as aspect_src, \\\n",
    "         rasterio.open(CURV_PATH) as curv_src, \\\n",
    "         rasterio.open(FLOWACC_PATH) as fa_src, \\\n",
    "         rasterio.open(TWI_PATH) as twi_src:\n",
    "\n",
    "        res_x, res_y = dem_src.res  # should be ~10,10 for DTM10\n",
    "        assert abs(res_x - res_y) < 1e-6\n",
    "        res = res_x\n",
    "\n",
    "        half_pixels = int((PATCH_SIZE_M / 2) / res)  \n",
    "        patch_pixels = half_pixels * 2               \n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting house stacks\"):\n",
    "            bid = int(row[\"bygningsnummer\"])\n",
    "            out_path = OUT_DIR / f\"house_{bid}.npy\"\n",
    "            if out_path.exists():\n",
    "                continue\n",
    "\n",
    "            geom = row.geometry\n",
    "\n",
    "            # Convert coord to raster indices\n",
    "            row_idx, col_idx = dem_src.index(geom.x, geom.y)\n",
    "\n",
    "            window = Window(\n",
    "                col_idx - half_pixels,\n",
    "                row_idx - half_pixels,\n",
    "                patch_pixels,\n",
    "                patch_pixels,\n",
    "            )\n",
    "\n",
    "            dem_patch    = dem_src.read(1, window=window)\n",
    "            slope_patch  = slope_src.read(1, window=window)\n",
    "            aspect_patch = aspect_src.read(1, window=window)\n",
    "            curv_patch   = curv_src.read(1, window=window)\n",
    "            fa_patch     = fa_src.read(1, window=window)\n",
    "            twi_patch    = twi_src.read(1, window=window)\n",
    "\n",
    "            if dem_patch.shape != (patch_pixels, patch_pixels):\n",
    "                # skip houses near the raster edge\n",
    "                continue\n",
    "\n",
    "            stack = np.stack(\n",
    "                [dem_patch, slope_patch, aspect_patch, curv_patch, fa_patch, twi_patch],\n",
    "                axis=0,\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "            # basic NaN handling\n",
    "            stack = np.where(np.isfinite(stack), stack, np.nan)\n",
    "            for c in range(stack.shape[0]):\n",
    "                chan = stack[c]\n",
    "                med = np.nanmedian(chan)\n",
    "                stack[c] = np.where(np.isnan(chan), med, chan)\n",
    "\n",
    "            stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "PATCH_DIR = DATA_ROOT / \"derived\" / \"dtm_patches\"\n",
    "FEATURE_DIR = Path(\"data/derived/dtm_features_ardal10m\")\n",
    "LABELS_PATH = DATA_ROOT / \"processed\" / \"features\" / \"houses_ardal_flom_labels.parquet\"\n",
    "STATS_PATH  = FEATURE_DIR / \"channel_stats.npz\"\n",
    "class FloodPatchDataset(Dataset):\n",
    "    def __init__(self, split: str = \"train\", normalize: bool = True):\n",
    "        df = pd.read_parquet(LABELS_PATH)\n",
    "        df = df[df[\"hazard_class_flom\"].notna()].copy()\n",
    "        df[\"hazard_class_flom\"] = df[\"hazard_class_flom\"].astype(int)\n",
    "\n",
    "        if \"split\" in df.columns:\n",
    "            df = df[df[\"split\"] == split]\n",
    "\n",
    "        df[\"feature_path\"] = df[\"bygningsnummer\"].astype(int).apply(\n",
    "            lambda bid: FEATURE_DIR / f\"house_{bid}.npy\"\n",
    "        )\n",
    "\n",
    "        mask = df[\"feature_path\"].apply(lambda p: p.exists())\n",
    "        missing = (~mask).sum()\n",
    "        if missing > 0:\n",
    "            print(f\"[WARN] {missing} feature stacks missing, dropping those rows\")\n",
    "        df = df[mask].reset_index(drop=True)\n",
    "\n",
    "        self.df = df\n",
    "        self.normalize = normalize\n",
    "\n",
    "        stats = np.load(STATS_PATH)\n",
    "        self.channel_mean = stats[\"mean\"]  # (C,)\n",
    "        self.channel_std  = stats[\"std\"]   # (C,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        arr = np.load(row[\"feature_path\"])  # (C,H,W)\n",
    "        if self.normalize:\n",
    "            mean = self.channel_mean[:, None, None]\n",
    "            std = self.channel_std[:, None, None]\n",
    "            arr = (arr - mean) / (std + 1e-6)\n",
    "        x = torch.from_numpy(arr).float()\n",
    "        y = torch.tensor(row[\"hazard_class_flom\"], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallFloodCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int = 6, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)  # (N,256)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = FloodPatchDataset(split=\"train\", normalize=True)\n",
    "val_ds   = FloodPatchDataset(split=\"val\", normalize=True)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- model / loss ---\n",
    "model = SmallFloodCNN(in_channels=6, num_classes=3).to(device)\n",
    "\n",
    "class_counts = train_ds.df[\"hazard_class_flom\"].value_counts()\n",
    "weights = torch.ones(3)\n",
    "if 0 in class_counts.index and 2 in class_counts.index:\n",
    "    weights[2] = class_counts[0] / class_counts[2]  # upweight rare high class\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"  val_loss={running_loss/len(val_loader):.4f}, val_acc={acc:.3f}\")\n",
    "    return acc\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_and_report(loader, model, device):\n",
    "    model.eval()\n",
    "    all_y = []\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_y.append(y.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_y)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "\n",
    "    print(\"Confusion matrix (rows=true, cols=pred, order [0,1,2]):\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]))\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, labels=[0,1,2]))\n",
    "\n",
    "evaluate_and_report(val_loader, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "FEATURE_DIR = Path(\"data/derived/dtm_features_ardal10m\")\n",
    "LABELS_PATH = DATA_ROOT / \"processed\" / \"features\" / \"houses_ardal_flom_labels.parquet\"\n",
    "STATS_PATH  = FEATURE_DIR / \"channel_stats.npz\"\n",
    "\n",
    "HOUSES_GPKG = Path(\"../data/processed/vector/exposure/houses_ardal_flom.gpkg\")\n",
    "HOUSES_LAYER = \"houses_flom\"\n",
    "\n",
    "OUT_GPKG = Path(\"data/processed/vector/exposure/houses_ardal_flom_preds.gpkg\")\n",
    "OUT_LAYER = \"houses_flom_preds_clean\"\n",
    "\n",
    "\n",
    "def build_preds_gpkg(model, device):\n",
    "    df = pd.read_parquet(LABELS_PATH)\n",
    "    df = df[df[\"hazard_class_flom\"].notna()].copy()\n",
    "    df[\"hazard_class_flom\"] = df[\"hazard_class_flom\"].astype(int)\n",
    "    df[\"feature_path\"] = df[\"bygningsnummer\"].astype(int).apply(\n",
    "        lambda bid: FEATURE_DIR / f\"house_{bid}.npy\"\n",
    "    )\n",
    "    df = df[df[\"feature_path\"].apply(lambda p: p.exists())].copy()\n",
    "\n",
    "    stats = np.load(STATS_PATH)\n",
    "    mean = stats[\"mean\"]  # (C,)\n",
    "    std  = stats[\"std\"]   # (C,)\n",
    "    mean_b = mean[:, None, None]\n",
    "    std_b  = std[:, None, None]\n",
    "\n",
    "    all_pred_class = []\n",
    "    all_p_high = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, row in df.iterrows():\n",
    "            arr = np.load(row[\"feature_path\"])  # raw (C,H,W)\n",
    "            # same normalization as in FloodPatchDataset\n",
    "            arr = (arr - mean_b) / (std_b + 1e-6)\n",
    "\n",
    "            x = torch.from_numpy(arr).unsqueeze(0).float().to(device)\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "            pred_cls = int(np.argmax(probs))\n",
    "            p_high = float(probs[2])  # prob of class 2\n",
    "\n",
    "            all_pred_class.append(pred_cls)\n",
    "            all_p_high.append(p_high)\n",
    "\n",
    "    df[\"pred_class_flom\"] = all_pred_class\n",
    "    df[\"pred_p_high\"] = all_p_high\n",
    "\n",
    "    houses = gpd.read_file(HOUSES_GPKG, layer=HOUSES_LAYER).to_crs(25833)\n",
    "    houses = houses[houses[\"hazard_class_flom\"].notna()].copy()\n",
    "    houses[\"hazard_class_flom\"] = houses[\"hazard_class_flom\"].astype(int)\n",
    "    merged = houses.merge(\n",
    "        df[[\"bygningsnummer\", \"pred_class_flom\", \"pred_p_high\"]],\n",
    "        on=\"bygningsnummer\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    OUT_GPKG.parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_file(OUT_GPKG, layer=OUT_LAYER, driver=\"GPKG\")\n",
    "    print(\"Wrote predictions to\", OUT_GPKG, \"layer\", OUT_LAYER)\n",
    "\n",
    "    mask = merged[\"pred_class_flom\"].notna()\n",
    "    acc = (merged.loc[mask, \"pred_class_flom\"] == merged.loc[mask, \"hazard_class_flom\"]).mean()\n",
    "    print(f\"Overall accuracy on all labelled houses: {acc:.3f} (should be close-ish to your val_acc)\")\n",
    "\n",
    "\n",
    "build_preds_gpkg(model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def subset_cols(df: gpd.GeoDataFrame, keep_cols: List[str]):\n",
    "    cols = [c for c in keep_cols if c in df.columns]\n",
    "    if \"geometry\" in df.columns and \"geometry\" not in cols:\n",
    "        cols.append(\"geometry\")\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def ensure_crs(gdf: gpd.GeoDataFrame, target_crs: str):\n",
    "    if gdf.crs is None or gdf.crs.to_string() != target_crs:\n",
    "        return gdf.to_crs(target_crs)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def simple_nan_fill_patch(patch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Patch: (C, H, W). Fill NaNs per-channel with that channel's median.\n",
    "    Very close to what you did earlier, but fast + reusable.\n",
    "    \"\"\"\n",
    "    x = patch.clone()\n",
    "    c = x.shape[0]\n",
    "    for i in range(c):\n",
    "        chan = x[i]\n",
    "        mask = torch.isfinite(chan)\n",
    "        if mask.any():\n",
    "            med = chan[mask].median()\n",
    "            chan = torch.where(mask, chan, med)\n",
    "        else:\n",
    "            chan = torch.zeros_like(chan)\n",
    "        x[i] = chan\n",
    "    return x\n",
    "\n",
    "\n",
    "class HouseRasterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    On-the-fly multi-channel raster patches + tabular + label.\n",
    "\n",
    "    houses_gdf must already include geometry + label + tabular columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        houses_gdf: gpd.GeoDataFrame,\n",
    "        raster_paths: Dict[str, Path],\n",
    "        patch_size_pixels: int,\n",
    "        tabular_cols: List[str],\n",
    "        label_col: str,\n",
    "        house_id_col: str = \"bygningsnummer\",\n",
    "        target_crs: str = \"EPSG:25833\",\n",
    "        transform_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "        fill_edges_with_nan: bool = True,\n",
    "    ):\n",
    "        houses_gdf = ensure_crs(houses_gdf, target_crs)\n",
    "\n",
    "        self.houses = houses_gdf.reset_index(drop=True)\n",
    "        self.raster_keys = list(raster_paths.keys())\n",
    "        self.rasters = {k: rasterio.open(str(p)) for k, p in raster_paths.items()}\n",
    "\n",
    "        self.patch_size = int(patch_size_pixels)\n",
    "        self.half = self.patch_size // 2\n",
    "\n",
    "        self.tabular_cols = tabular_cols\n",
    "        self.label_col = label_col\n",
    "        self.house_id_col = house_id_col\n",
    "        self.transform_fn = transform_fn\n",
    "        self.fill_edges_with_nan = fill_edges_with_nan\n",
    "\n",
    "        # sanity: check all rasters share CRS/resolution-ish\n",
    "        ref = next(iter(self.rasters.values()))\n",
    "        self.ref_crs = ref.crs\n",
    "        self.ref_res = ref.res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.houses)\n",
    "\n",
    "    def close(self):\n",
    "        for src in self.rasters.values():\n",
    "            try:\n",
    "                src.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def _window_for_point(self, src: rasterio.io.DatasetReader, x: float, y: float):\n",
    "        row_idx, col_idx = src.index(x, y)\n",
    "        return Window(\n",
    "            col_off=col_idx - self.half,\n",
    "            row_off=row_idx - self.half,\n",
    "            width=self.patch_size,\n",
    "            height=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def _read_multichannel_patch(self, geom) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns (C,H,W) float32 tensor.\n",
    "        Uses boundless read to avoid edge crashes.\n",
    "        \"\"\"\n",
    "        patches = []\n",
    "        for k in self.raster_keys:\n",
    "            src = self.rasters[k]\n",
    "            win = self._window_for_point(src, geom.x, geom.y)\n",
    "\n",
    "            arr = src.read(\n",
    "                1,\n",
    "                window=win,\n",
    "                boundless=True,\n",
    "                fill_value=np.nan if self.fill_edges_with_nan else 0,\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "            patches.append(arr)\n",
    "\n",
    "        stack = np.stack(patches, axis=0).astype(\"float32\")\n",
    "        return torch.from_numpy(stack)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.houses.iloc[idx]\n",
    "        geom = row.geometry\n",
    "\n",
    "        patch = self._read_multichannel_patch(geom)\n",
    "\n",
    "        # optional quick NaN handling\n",
    "        patch = simple_nan_fill_patch(patch)\n",
    "\n",
    "        if self.transform_fn:\n",
    "            patch = self.transform_fn(patch)\n",
    "\n",
    "        tab_vals = row[self.tabular_cols].values.astype(np.float32)\n",
    "        tab_tensor = torch.from_numpy(tab_vals)\n",
    "\n",
    "        label = int(row[self.label_col])\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        house_id = row.get(self.house_id_col, idx)\n",
    "\n",
    "        return {\n",
    "            \"patch\": patch,\n",
    "            \"tabular\": tab_tensor,\n",
    "            \"label\": label_tensor,\n",
    "            \"house_id\": house_id,\n",
    "        }\n",
    "\n",
    "def compute_channel_stats(\n",
    "    houses_gdf: gpd.GeoDataFrame,\n",
    "    raster_paths: Dict[str, Path],\n",
    "    patch_size_pixels: int,\n",
    "    target_crs: str = \"EPSG:25833\",\n",
    "    sample_n: int = 2000,\n",
    "    seed: int = 42,\n",
    "    out_path: Optional[str | Path] = None,\n",
    "):\n",
    "    houses_gdf = ensure_crs(houses_gdf, target_crs)\n",
    "\n",
    "    if len(houses_gdf) == 0:\n",
    "        raise ValueError(\"houses_gdf is empty\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_n, len(houses_gdf))\n",
    "    sample_idx = rng.choice(len(houses_gdf), size=n, replace=False)\n",
    "\n",
    "    raster_keys = list(raster_paths.keys())\n",
    "    rasters = {k: rasterio.open(str(p)) for k, p in raster_paths.items()}\n",
    "\n",
    "    half = patch_size_pixels // 2\n",
    "    c = len(raster_keys)\n",
    "\n",
    "    # Welford stats per channel\n",
    "    count = np.zeros(c, dtype=np.int64)\n",
    "    mean = np.zeros(c, dtype=np.float64)\n",
    "    m2 = np.zeros(c, dtype=np.float64)\n",
    "\n",
    "    def window_for_point(src, x, y):\n",
    "        row_idx, col_idx = src.index(x, y)\n",
    "        return Window(col_idx - half, row_idx - half, patch_size_pixels, patch_size_pixels)\n",
    "\n",
    "    try:\n",
    "        for i in sample_idx:\n",
    "            geom = houses_gdf.iloc[i].geometry\n",
    "\n",
    "            for ci, k in enumerate(raster_keys):\n",
    "                src = rasters[k]\n",
    "                win = window_for_point(src, geom.x, geom.y)\n",
    "\n",
    "                arr = src.read(\n",
    "                    1,\n",
    "                    window=win,\n",
    "                    boundless=True,\n",
    "                    fill_value=np.nan,\n",
    "                ).astype(\"float32\")\n",
    "\n",
    "                vals = arr[np.isfinite(arr)]\n",
    "                if vals.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # update channel stats with batch of values\n",
    "                for v in vals:\n",
    "                    count[ci] += 1\n",
    "                    delta = v - mean[ci]\n",
    "                    mean[ci] += delta / count[ci]\n",
    "                    delta2 = v - mean[ci]\n",
    "                    m2[ci] += delta * delta2\n",
    "\n",
    "    finally:\n",
    "        for src in rasters.values():\n",
    "            try:\n",
    "                src.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # avoid divide-by-zero\n",
    "    var = np.where(count > 1, m2 / (count - 1), 0.0)\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    stats = {\n",
    "        \"keys\": np.array(raster_keys),\n",
    "        \"mean\": mean.astype(\"float32\"),\n",
    "        \"std\": std.astype(\"float32\"),\n",
    "        \"count\": count,\n",
    "        \"patch_size_pixels\": np.array([patch_size_pixels]),\n",
    "        \"sample_n\": np.array([n]),\n",
    "    }\n",
    "\n",
    "    if out_path is not None:\n",
    "        out_path = Path(out_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.savez(out_path, **stats)\n",
    "        print(\"Wrote channel stats:\", out_path)\n",
    "        print(\"keys:\", raster_keys)\n",
    "        print(\"mean:\", stats[\"mean\"])\n",
    "        print(\"std:\", stats[\"std\"])\n",
    "\n",
    "    return stats\n",
    "\n",
    "class NormalizeChannels:\n",
    "    def __init__(self, stats_path: str | Path):\n",
    "        d = np.load(stats_path, allow_pickle=True)\n",
    "        self.keys = list(d[\"keys\"])\n",
    "        self.mean = torch.tensor(d[\"mean\"], dtype=torch.float32).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(d[\"std\"], dtype=torch.float32).view(-1, 1, 1)\n",
    "        self.std = torch.where(self.std == 0, torch.ones_like(self.std), self.std)\n",
    "\n",
    "    def __call__(self, patch: torch.Tensor):\n",
    "        # patch (C,H,W)\n",
    "        return (patch - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "regions = [\"sogn\", \"lillehammer\", \"oslo\"]\n",
    "\n",
    "tab_cols = [\n",
    "    \"dist_to_river\",\n",
    "    \"dist_to_lake\",\n",
    "    \"dist_to_hyd\",\n",
    "    \"QNormalOppstrm_Mm3Aar\",\n",
    "    \"QNormal_lskm2\",\n",
    "    \"QNormal_Mm3Aar\",\n",
    "    \"dist_to_trigger_point\",\n",
    "    \"dist_to_runout_area\",\n",
    "    \"dist_to_runout_point\",\n",
    "    \"dist_to_landslide_event\",\n",
    "    \"dist_to_source_area\"\n",
    "    # add your other numeric cols here\n",
    "]\n",
    "def default_raster_paths_for_region(\n",
    "    region: str,\n",
    "    processed_raster_dir: str | Path,\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Simple convention-based builder.\n",
    "    Adjust names if your files differ.\n",
    "    \"\"\"\n",
    "    processed_raster_dir = Path(processed_raster_dir)\n",
    "\n",
    "    return {\n",
    "        \"dtm\": processed_raster_dir / f\"{region}_dtm10_filled.tif\",\n",
    "        \"slope\": processed_raster_dir / f\"{region}_slope_deg.tif\",\n",
    "        \"aspect\": processed_raster_dir / f\"{region}_aspect_deg.tif\",\n",
    "        \"curv\": processed_raster_dir / f\"{region}_curvature.tif\",\n",
    "        \"flowacc\": processed_raster_dir / f\"{region}_flowacc_d8.tif\",\n",
    "        \"twi\": processed_raster_dir / f\"{region}_twi.tif\",\n",
    "    }\n",
    "def split_houses_train_val(\n",
    "    houses: gpd.GeoDataFrame,\n",
    "    label_col: str,\n",
    "    val_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "    stratify: bool = True,\n",
    "):\n",
    "    if len(houses) == 0:\n",
    "        raise ValueError(\"No houses to split.\")\n",
    "\n",
    "    if label_col not in houses.columns:\n",
    "        raise ValueError(f\"Label col '{label_col}' not found in houses.\")\n",
    "\n",
    "    if train_test_split is None:\n",
    "        # simple fallback without sklearn\n",
    "        houses = houses.sample(frac=1.0, random_state=seed).copy()\n",
    "        n_val = int(len(houses) * val_size)\n",
    "        val = houses.iloc[:n_val].copy()\n",
    "        train = houses.iloc[n_val:].copy()\n",
    "        return train, val\n",
    "\n",
    "    y = houses[label_col].astype(\"int64\", errors=\"ignore\")\n",
    "\n",
    "    # If stratify requested but classes too tiny, fallback gracefully.\n",
    "    strat = y if stratify else None\n",
    "    try:\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            houses.index.to_numpy(),\n",
    "            test_size=val_size,\n",
    "            random_state=seed,\n",
    "            stratify=strat,\n",
    "        )\n",
    "    except Exception:\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            houses.index.to_numpy(),\n",
    "            test_size=val_size,\n",
    "            random_state=seed,\n",
    "            stratify=None,\n",
    "        )\n",
    "\n",
    "    train = houses.loc[train_idx].copy()\n",
    "    val = houses.loc[val_idx].copy()\n",
    "    return train, val\n",
    "\n",
    "def make_region_dataset(\n",
    "    region: str,\n",
    "    houses_gpkg: str | Path,\n",
    "    houses_layer: str,\n",
    "    tabular_cols: List[str],\n",
    "    label_col=\"hazard_class_flom\",\n",
    "    patch_size_pixels = 20,\n",
    "    house_id_col: str = \"bygningsnummer\",\n",
    "    target_crs: str = \"EPSG:25833\",\n",
    "    raster_paths: Optional[Dict[str, Path]] = None,\n",
    "    processed_raster_dir: str | Path = \"../processed/rasters\",\n",
    "    compute_stats: bool = True,\n",
    "    stats_out_dir: str | Path = \"../processed/channel_stats\",\n",
    "    stats_sample_n: int = 1500,\n",
    "    normalize: bool = True,\n",
    "    val_size: float = 0.2,\n",
    "    stratify: bool = True,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dataset, stats_path (or None), houses_gdf\n",
    "    \"\"\"\n",
    "    # ------------------\n",
    "    # 1) Load houses\n",
    "    # ------------------\n",
    "    houses = gpd.read_file(houses_gpkg, layer=houses_layer)\n",
    "    houses = ensure_crs(houses, target_crs)\n",
    "    f_links = pd.read_parquet(f\"../processed/links/flood_links_{region}.parquet\")\n",
    "    landslide_links = pd.read_parquet(f\"../processed/links/landslide_links_closest_{region}.parquet\")\n",
    "    hydro_links = pd.read_parquet(f\"../processed/links/hydro_links_{region}.parquet\")\n",
    "    \n",
    "    # add empty tabular columns to houses\n",
    "    houses = subset_cols(houses, [house_id_col, label_col] + tabular_cols)\n",
    "    \n",
    "    f_links = f_links[[c for c in f_links.columns if c in (tabular_cols + [house_id_col, label_col])]].copy()\n",
    "    landslide_links = landslide_links[[c for c in landslide_links.columns if c in (tabular_cols + [house_id_col])]].copy()\n",
    "    hydro_links = hydro_links[[c for c in hydro_links.columns if c in (tabular_cols + [house_id_col])]].copy()\n",
    "    \n",
    "    houses = houses.merge(f_links, on=house_id_col, how=\"left\", suffixes=(\"\", \"_flood\"))\n",
    "    houses = houses.merge(landslide_links, on=house_id_col, how=\"left\", suffixes=(\"\", \"_landslide\"))\n",
    "    houses = houses.merge(hydro_links, on=house_id_col, how=\"left\", suffixes=(\"\", \"_hydro\"))\n",
    "    \n",
    "    if label_col not in houses.columns:\n",
    "        raise ValueError(f\"label_col='{label_col}' not found in houses layer\")\n",
    "\n",
    "    houses = houses[houses[label_col].notna()].copy()\n",
    "    if len(houses) == 0:\n",
    "        raise ValueError(f\"No houses with non-null {label_col} for region={region}\")\n",
    "\n",
    "    # ------------------\n",
    "    # 2) Resolve raster paths\n",
    "    # ------------------\n",
    "    if raster_paths is None:\n",
    "        raster_paths = default_raster_paths_for_region(\n",
    "            region=region,\n",
    "            processed_raster_dir=processed_raster_dir,\n",
    "        )\n",
    "\n",
    "    # quick existence check\n",
    "    missing = [k for k, p in raster_paths.items() if not Path(p).exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing raster files for region={region}: {missing}\\n\"\n",
    "            f\"Paths: { {k: str(v) for k, v in raster_paths.items()} }\"\n",
    "        )\n",
    "\n",
    "    houses_train, houses_val = split_houses_train_val(\n",
    "        houses=houses,\n",
    "        label_col=label_col,\n",
    "        val_size=val_size,\n",
    "        seed=seed,\n",
    "        stratify=stratify,\n",
    "    )\n",
    "    # ------------------\n",
    "    # 3) Compute / locate stats\n",
    "    # ------------------\n",
    "    stats_out_dir = Path(stats_out_dir)\n",
    "    stats_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stats_path = stats_out_dir / f\"{region}_channel_stats.npz\"\n",
    "\n",
    "    if not stats_path.exists():\n",
    "        compute_channel_stats(\n",
    "            houses_gdf=houses_train,\n",
    "            raster_paths=raster_paths,\n",
    "            patch_size_pixels=patch_size_pixels,\n",
    "            target_crs=target_crs,\n",
    "            sample_n=stats_sample_n,\n",
    "            seed=seed,\n",
    "            out_path=stats_path,\n",
    "        )\n",
    "\n",
    "    # ------------------\n",
    "    # 4) Optional normalization transform\n",
    "    # ------------------\n",
    "    transform_fn = None\n",
    "    if normalize:\n",
    "        if stats_path is None:\n",
    "            raise ValueError(\"normalize=True but compute_stats=False and no stats_path available\")\n",
    "        transform_fn = NormalizeChannels(stats_path)\n",
    "\n",
    "    # ------------------\n",
    "    # 5) Build dataset\n",
    "    # ------------------\n",
    "    train_ds = HouseRasterDataset(\n",
    "        houses_gdf=houses_train,\n",
    "        raster_paths=raster_paths,\n",
    "        patch_size_pixels=patch_size_pixels,\n",
    "        tabular_cols=tabular_cols,\n",
    "        label_col=label_col,\n",
    "        house_id_col=house_id_col,\n",
    "        target_crs=target_crs,\n",
    "        transform_fn=transform_fn,\n",
    "    )\n",
    "\n",
    "    val_ds = HouseRasterDataset(\n",
    "        houses_gdf=houses_val,\n",
    "        raster_paths=raster_paths,\n",
    "        patch_size_pixels=patch_size_pixels,\n",
    "        tabular_cols=tabular_cols,\n",
    "        label_col=label_col,\n",
    "        house_id_col=house_id_col,\n",
    "        target_crs=target_crs,\n",
    "        transform_fn=transform_fn,\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds, stats_path, houses_train, houses_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"sogn\", \"lillehammer\", \"oslo\"]\n",
    "\n",
    "tab_cols = [\n",
    "    \"dist_to_river\",\n",
    "    \"dist_to_lake\",\n",
    "    \"dist_to_hyd\",\n",
    "]\n",
    "\n",
    "def build_all_regions_train(\n",
    "    houses_gpkg_tpl: str | Path,\n",
    "    houses_layer_tpl: str,\n",
    "    batch_size: int = 8,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    "    shuffle_train: bool = True,\n",
    "    raster_paths_by_region: Optional[Dict[str, Dict[str, Path]]] = None,\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "    results: Dict[str, Dict[str, Any]] = {}\n",
    "    regions = [\"sogn\", \"lillehammer\", \"oslo\"]\n",
    "\n",
    "    tab_cols = [\n",
    "        \"dist_to_river\",\n",
    "        \"dist_to_lake\",\n",
    "        \"dist_to_hyd\",\n",
    "        \"QNormalOppstrm_Mm3Aar\",\n",
    "        \"QNormal_lskm2\",\n",
    "        \"QNormal_Mm3Aar\",\n",
    "        \"dist_to_trigger_point\",\n",
    "        \"dist_to_runout_area\",\n",
    "        \"dist_to_runout_point\",\n",
    "        \"dist_to_landslide_event\",\n",
    "        \"dist_to_source_area\"\n",
    "    ]\n",
    "    houses_gpkg_tpl = str(houses_gpkg_tpl)\n",
    "    houses_layer_tpl = str(houses_layer_tpl)\n",
    "\n",
    "    for region in regions:\n",
    "        houses_gpkg = Path(houses_gpkg_tpl.format(region=region))\n",
    "        houses_layer = houses_layer_tpl.format(region=region)\n",
    "        train_ds, val_ds, stats_path, h_train, h_val = make_region_dataset(\n",
    "            region=region,\n",
    "            houses_gpkg=houses_gpkg,\n",
    "            houses_layer=houses_layer,\n",
    "            tabular_cols=tab_cols,\n",
    "            label_col=\"hazard_class_flom\",\n",
    "            patch_size_pixels=20,   # 200m @10m\n",
    "            processed_raster_dir=\"../processed/rasters\",\n",
    "            stats_out_dir=\"../processed/channel_stats\",\n",
    "            stats_sample_n=1500,\n",
    "            normalize=True,\n",
    "            val_size=0.2,\n",
    "            stratify=True,\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_train,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "        results[region] = {\n",
    "            \"train_ds\": train_ds,\n",
    "            \"val_ds\": val_ds,\n",
    "            \"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"stats_path\": stats_path,\n",
    "            \"houses_train\": h_train,\n",
    "            \"houses_val\": h_val,\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"[{region}] \"\n",
    "            f\"train={len(train_ds)} val={len(val_ds)} \"\n",
    "            f\"stats={Path(stats_path).name}\"\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = build_all_regions_train(\n",
    "    houses_gpkg_tpl=\"../raw/vector/houses/houses_{region}.gpkg\",\n",
    "    houses_layer_tpl=\"houses_{region}\",\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def concat_class_counts(concat_ds, label_col=\"hazard_class_flom\"):\n",
    "    \"\"\"\n",
    "    Build class counts from a ConcatDataset by reading each sub-dataset's df.\n",
    "    Supports ds.houses_df or ds.df.\n",
    "    \"\"\"\n",
    "    series_list = []\n",
    "    for ds in concat_ds.datasets:\n",
    "        if hasattr(ds, \"houses\"):\n",
    "            series_list.append(ds.houses[label_col])\n",
    "        elif hasattr(ds, \"df\"):\n",
    "            series_list.append(ds.df[label_col])\n",
    "        else:\n",
    "            raise AttributeError(\"Sub-dataset missing houses_df/df for label counts.\")\n",
    "\n",
    "    labels = pd.concat(series_list, ignore_index=True)\n",
    "    return labels.value_counts()\n",
    "\n",
    "def make_class_weights_from_counts(class_counts, num_classes=3):\n",
    "    \"\"\"\n",
    "    Simple inverse-frequency-ish weights.\n",
    "    \"\"\"\n",
    "    weights = torch.ones(num_classes, dtype=torch.float32)\n",
    "    total = class_counts.sum()\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        if c in class_counts.index and class_counts[c] > 0:\n",
    "            weights[c] = total / (num_classes * class_counts[c])\n",
    "\n",
    "    weights = weights / weights.mean()\n",
    "    return weights\n",
    "\n",
    "def unpack_batch(batch, device):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      - dict batch: {\"patch\": ..., \"label\": ...}\n",
    "      - tuple batch: (x, y)\n",
    "    \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        x = batch[\"patch\"].to(device)\n",
    "        y = batch[\"label\"].to(device)\n",
    "        return x, y\n",
    "\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        return x, y\n",
    "\n",
    "    raise ValueError(\"Unknown batch format. Expected dict or (x, y).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "regions = [\"sogn\", \"lillehammer\", \"oslo\"]\n",
    "\n",
    "all_train = ConcatDataset([bundle[r][\"train_ds\"] for r in regions])\n",
    "all_val   = ConcatDataset([bundle[r][\"val_ds\"] for r in regions])\n",
    "\n",
    "train_loader = DataLoader(all_train, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(all_val, batch_size=8, shuffle=False)\n",
    "model = SmallFloodCNN(in_channels=6, num_classes=3).to(device)\n",
    "\n",
    "class_counts = concat_class_counts(all_train, label_col=\"hazard_class_flom\")\n",
    "weights = make_class_weights_from_counts(class_counts, num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y = unpack_batch(batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, return_preds=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y = unpack_batch(batch, device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if return_preds:\n",
    "            all_preds.append(preds.detach().cpu())\n",
    "            all_targets.append(y.detach().cpu())\n",
    "\n",
    "    avg_loss = running_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "\n",
    "    if return_preds:\n",
    "        return avg_loss, acc, torch.cat(all_preds), torch.cat(all_targets)\n",
    "\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 21):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, preds, targets = evaluate(\n",
    "    model, val_loader, criterion, device, return_preds=True\n",
    "    )\n",
    "\n",
    "    # preds/targets are torch tensors on CPU\n",
    "    torch.save(\n",
    "        {\n",
    "            \"preds\": preds,\n",
    "            \"targets\": targets,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        },\n",
    "        \"val_predictions.pt\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "georisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
